services:
  db:
    image: postgres:16
    restart: unless-stopped
    environment:
      POSTGRES_USER: raido
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: raido
    volumes: [db:/var/lib/postgresql/data]

  db-admin:
    image: dpage/pgadmin4:latest
    restart: unless-stopped
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@raido.local
      PGADMIN_DEFAULT_PASSWORD: ${POSTGRES_PASSWORD}
    ports: ["5050:80"]
    depends_on: [db]

  icecast:
    image: infiniteproject/icecast
    restart: unless-stopped
    ports: ["8000:8000"]
    environment:
      - ICECAST_SOURCE_PASSWORD=hackme
      - ICECAST_ADMIN_PASSWORD=hackme
      - ICECAST_PASSWORD=hackme
      - ICECAST_RELAY_PASSWORD=hackme
    volumes:
      - ./logs/icecast:/var/log/icecast

  liquidsoap:
    image: savonet/liquidsoap:v2.2.5
    restart: unless-stopped
    depends_on: [icecast]
    volumes:
      - ./music:/music:ro
      - ./music:/mnt/music:ro
      - ./infra/liquidsoap/radio.liq:/radio.liq:ro
      - shared:/shared
    command: ["liquidsoap","/radio.liq"]
    ports: ["1234:1234","8080:8080"]

  api:
    build: ./services/api
    restart: unless-stopped
    env_file: .env
    depends_on: [db]
    volumes:
      - shared:/shared
      - ./music:/music:ro
      - ./music:/mnt/music:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # ollama:
  #   image: ollama/ollama:latest
  #   restart: unless-stopped
  #   volumes: 
  #     - ollama:/root/.ollama
  #   environment:
  #     - OLLAMA_HOST=0.0.0.0:11434
  #     - OLLAMA_NUM_PARALLEL=1
  #     - OLLAMA_MAX_LOADED_MODELS=1
  #     - OLLAMA_FLASH_ATTENTION=1
  #     - OLLAMA_KV_CACHE_TYPE=f16
  #   mem_limit: "4g"
  #   cpus: "2.0"

  kokoro-tts:
    image: ghcr.io/remsky/kokoro-fastapi-cpu
    restart: unless-stopped
    # Remap host port to avoid conflicts; service remains on 8880 internally
    ports:
      - "8091:8880"
    environment:
      - USE_GPU=false
      - DOWNLOAD_MODEL=true
    # Constrain CPU and memory to avoid host contention
    mem_limit: "3g"
    cpus: "1.0"

  # xtts-server:  # Temporarily disabled to reduce system load during Chatterbox testing
  #   image: synesthesiam/opentts:all
  #   restart: unless-stopped
  #   # OpenTTS listens on 5500 internally. We don't need to expose to host; other
  #   # services access it via Docker network at http://xtts-server:5500
  #   # Note: XTTS support has been removed from this codebase.
  #   environment:
  #     # Limit to specific TTS engines to prevent multiple model loading
  #     - TTS_ENGINES=coqui-tts:en_vctk,coqui-tts:en_ljspeech
  #     # Prevent concurrent model loading
  #     - MAX_CONCURRENT_REQUESTS=1
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 3G  # Limit container to 3GB RAM
  #       reservations:
  #         memory: 1G  # Reserve at least 1GB RAM

  dj-worker:
    build: ./services/dj-worker
    restart: unless-stopped
    env_file: .env
    depends_on:
      api:
        condition: service_healthy
    volumes: [shared:/shared]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://api:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    mem_limit: "1g"
    cpus: "0.50"

  chatterbox-shim:
    build: ./services/chatterbox-shim
    restart: unless-stopped
    environment:
      # Route shim to external Chatterbox server (not the local container)
      - CH_SHIM_UPSTREAM=http://192.168.1.112:8000
      - CH_SHIM_TIMEOUT=120
      - CH_SHIM_FORCE_MP3=true
    ports:
      - "18000:8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 20s
      timeout: 5s
      retries: 5
      start_period: 20s

  proxy:
    image: caddy:2
    restart: unless-stopped
    ports: ["80:80","443:443"]
    volumes:
      - ./infra/Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy:/data
      - shared:/shared
    depends_on: [api, icecast]

  web:
    build: ./web
    restart: unless-stopped
    depends_on: [api]

  monitor:
    image: curlimages/curl:8.8.0
    restart: unless-stopped
    depends_on: [api, web, icecast]
    env_file: .env
    volumes:
      - ./infra/monitor/healthcheck.sh:/healthcheck.sh:ro
    command: ["sh","/healthcheck.sh"]

  tts-cleanup:
    image: alpine:3.18
    restart: unless-stopped
    volumes:
      - shared:/shared
      - ./scripts/cleanup-tts.sh:/cleanup-tts.sh:ro
    command: ["sh", "-c", "while true; do sleep 86400; /cleanup-tts.sh; done"]

volumes: { db: {}, caddy: {}, shared: {}, kokoro_models: {} }
