services:
  db:
    image: postgres:16
    restart: unless-stopped
    environment:
      POSTGRES_USER: raido
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: raido
    volumes: [db:/var/lib/postgresql/data]

  db-admin:
    image: dpage/pgadmin4:latest
    restart: unless-stopped
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@raido.local
      PGADMIN_DEFAULT_PASSWORD: ${POSTGRES_PASSWORD}
    ports: ["5050:80"]
    depends_on: [db]

  icecast:
    image: infiniteproject/icecast
    restart: unless-stopped
    ports: ["8000:8000"]
    environment:
      - ICECAST_SOURCE_PASSWORD=hackme
      - ICECAST_ADMIN_PASSWORD=hackme
      - ICECAST_PASSWORD=hackme
      - ICECAST_RELAY_PASSWORD=hackme
    volumes:
      - ./logs/icecast:/var/log/icecast

  liquidsoap:
    image: savonet/liquidsoap:v2.2.5
    restart: unless-stopped
    depends_on: [icecast]
    volumes:
      - /mnt/music:/mnt/music:ro
      - ./infra/liquidsoap/radio.liq:/radio.liq:ro
      - shared:/shared
    command: ["liquidsoap","/radio.liq"]
    ports: ["1234:1234","8080:8080"]

  christmas-liquidsoap:
    image: savonet/liquidsoap:v2.2.5
    restart: unless-stopped
    depends_on: [icecast]
    volumes:
      - /mnt/music:/mnt/music:ro
      - ./infra/liquidsoap/christmas.liq:/christmas.liq:ro
      - shared:/shared
    command: ["liquidsoap","/christmas.liq"]
    ports: ["1235:1235"]

  api:
    build: ./services/api
    restart: unless-stopped
    env_file: .env
    depends_on: [db]
    volumes: 
      - shared:/shared
      - /mnt/music:/mnt/music:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # ollama:
  #   image: ollama/ollama:latest
  #   restart: unless-stopped
  #   volumes: 
  #     - ollama:/root/.ollama
  #   environment:
  #     - OLLAMA_HOST=0.0.0.0:11434
  #     - OLLAMA_NUM_PARALLEL=1
  #     - OLLAMA_MAX_LOADED_MODELS=1
  #     - OLLAMA_FLASH_ATTENTION=1
  #     - OLLAMA_KV_CACHE_TYPE=f16
  #   mem_limit: "4g"
  #   cpus: "2.0"

  kokoro-tts:
    image: ghcr.io/remsky/kokoro-fastapi-cpu
    restart: unless-stopped
    # Remap host port to avoid conflicts; service remains on 8880 internally
    ports:
      - "8091:8880"
    environment:
      - USE_GPU=false
      - DOWNLOAD_MODEL=true
    # Constrain CPU and memory to avoid host contention
    mem_limit: "3g"
    cpus: "2.0"
    # Use tmpfs for /tmp to prevent temp file accumulation on disk
    # Files are stored in RAM and auto-cleaned on container restart
    # Use 'exec' to allow loading shared libraries (espeak-ng.so)
    tmpfs:
      - /tmp:size=512m,mode=1777,exec

  # xtts-server:  # Temporarily disabled to reduce system load during Chatterbox testing
  #   image: synesthesiam/opentts:all
  #   restart: unless-stopped
  #   # OpenTTS listens on 5500 internally. We don't need to expose to host; other
  #   # services access it via Docker network at http://xtts-server:5500
  #   # Note: XTTS support has been removed from this codebase.
  #   environment:
  #     # Limit to specific TTS engines to prevent multiple model loading
  #     - TTS_ENGINES=coqui-tts:en_vctk,coqui-tts:en_ljspeech
  #     # Prevent concurrent model loading
  #     - MAX_CONCURRENT_REQUESTS=1
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 3G  # Limit container to 3GB RAM
  #       reservations:
  #         memory: 1G  # Reserve at least 1GB RAM

  dj-worker:
    build: ./services/dj-worker
    restart: unless-stopped
    env_file: .env
    depends_on:
      api:
        condition: service_healthy
    volumes: [shared:/shared]
    environment:
      - STATION_NAME=main
      - LIQUIDSOAP_HOST=liquidsoap
      - LIQUIDSOAP_PORT=1234
    healthcheck:
      test: ["CMD", "curl", "-f", "http://api:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    mem_limit: "1g"
    cpus: "0.50"

  christmas-dj-worker:
    build: ./services/dj-worker
    restart: unless-stopped
    env_file: .env
    depends_on:
      api:
        condition: service_healthy
    volumes: [shared:/shared]
    environment:
      - STATION_NAME=christmas
      - LIQUIDSOAP_HOST=christmas-liquidsoap
      - LIQUIDSOAP_PORT=1235
    healthcheck:
      test: ["CMD", "curl", "-f", "http://api:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    mem_limit: "1g"
    cpus: "0.50"

  mb-enricher:
    build: ./services/mb-enricher
    restart: unless-stopped
    env_file: .env
    depends_on:
      api:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 20s
    mem_limit: "256m"
    cpus: "0.25"

  chatterbox-shim:
    build: ./services/chatterbox-shim
    restart: unless-stopped
    environment:
      # Primary: MacBook chatterbox (LAN IP), Fallback: local Kokoro
      - CH_SHIM_UPSTREAM=http://192.168.1.170:8150,http://kokoro-tts:8880
      - CH_SHIM_TIMEOUT=120
      - CH_SHIM_FORCE_MP3=true
    ports:
      - "18000:8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 20s
      timeout: 5s
      retries: 5
      start_period: 20s

  proxy:
    image: caddy:2
    restart: unless-stopped
    ports: ["80:80","443:443","8888:8888"]
    volumes:
      - ./infra/Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy:/data
      - shared:/shared
    depends_on: [api, icecast]

  web:
    build: ./web
    restart: unless-stopped
    depends_on: [api]

  christmas-web:
    build: ./web-christmas
    restart: unless-stopped
    depends_on: [api]
    ports: ["9000:80"]

  monitor:
    image: curlimages/curl:8.8.0
    restart: unless-stopped
    depends_on: [api, web, icecast]
    env_file: .env
    volumes:
      - ./infra/monitor/healthcheck.sh:/healthcheck.sh:ro
    command: ["sh","/healthcheck.sh"]

  tts-cleanup:
    image: alpine:3.18
    restart: unless-stopped
    volumes:
      - shared:/shared
      - ./scripts/cleanup-tts.sh:/cleanup-tts.sh:ro
    command: ["sh", "-c", "while true; do sleep 86400; /cleanup-tts.sh; done"]

  mcp:
    build: ./services/mcp
    restart: unless-stopped
    depends_on:
      api:
        condition: service_healthy
    ports:
      - "8811:8811"
    environment:
      - RAIDO_API_URL=http://api:8000

  # ─── Voice of Raido — Commentary Pre-rendering Engine ─────────────────────
  voicing-worker:
    build: ./services/dj-worker
    restart: unless-stopped
    command: ["python", "-m", "app.voicing_main"]
    env_file: .env
    depends_on:
      api:
        condition: service_healthy
    volumes: [shared:/shared]
    environment:
      - STATION_NAME=voicing-engine
      # ANTHROPIC_API_KEY must be set in .env
    # Run with conservative resource limits — this is a background job
    mem_limit: "512m"
    cpus: "0.25"

  # ─── Download Gateway — Lidarr music management ───────────────────────────
  lidarr:
    image: lscr.io/linuxserver/lidarr:latest
    restart: unless-stopped
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=UTC
    volumes:
      - lidarr_config:/config
      # Music library — Lidarr imports completed downloads here instantly
      - /mnt/music:/mnt/music
      # Downloads from the Gateway LXC (mount via NFS or bind if co-located)
      # - /mnt/downloads:/mnt/downloads
    ports:
      - "8686:8686"
    # Lidarr does NOT go through the VPN — it talks to Prowlarr/qBittorrent
    # on the Gateway LXC via LAN IP (192.168.1.201). See infra/gateway/VPN_GATEWAY.md

volumes: { db: {}, caddy: {}, shared: {}, kokoro_models: {}, lidarr_config: {} }
